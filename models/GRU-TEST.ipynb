{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84aca87f-fe47-4e42-bf02-baaf0a4833af",
   "metadata": {},
   "source": [
    "# GRU Model\n",
    "## LOAD IN TWO DATA SETS FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4947340d-03cb-4ad8-8c38-0cd14c6500be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Masking, InputLayer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense, Masking, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters, RandomSearch, Objective\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Local Path\n",
    "data_path = r'..\\data'\n",
    "\n",
    "# Files to Read\n",
    "complete_data_files = [i for i in listdir(data_path) if ('model' in i) & ('complete' in i)]\n",
    "imputed_data_files = [i for i in listdir(data_path) if ('model' in i) & ('impute' in i)]\n",
    "\n",
    "# Read and concat data\n",
    "complete_data = pd.concat([pd.read_csv(data_path + '\\\\' + file) for file in complete_data_files]).reset_index(drop=True).drop(columns=['time_bucket'])\n",
    "imputed_data = pd.concat([pd.read_csv(data_path + '\\\\' + file) for file in imputed_data_files]).reset_index(drop=True).drop(columns=['time_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bb6ccd-d6cb-43aa-bcfc-92761975a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stratification(data):\n",
    "    # Sort and group data by 'Unique Stay' and 'sequence_num' to ensure time order\n",
    "    df = data.sort_values(by=['stay_id', 'seq_num'])\n",
    "    \n",
    "    # 2. Normalize Numerical Columns\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['map', 'hr', 'pao2', 'fio2', 'creatinine', 'lactate', 'platelets', 'gcs']\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 3. Encode Categorical Variables\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_cols = ['gender', 'race', 'marital_status', 'insurance']\n",
    "    encoded_cats = encoder.fit_transform(df[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "    \n",
    "    # 4. Group by stay_id and Create Sequences\n",
    "    grouped = df.groupby('stay_id')\n",
    "    sequences = [group.drop(columns=['stay_id', 'mortality']).values for _, group in grouped]\n",
    "    labels = [group['mortality'].iloc[0] for _, group in grouped]\n",
    "    \n",
    "    # 5. Convert Labels to NumPy array for Stratified Splitting\n",
    "    labels = np.array(labels).astype('float32')\n",
    "    \n",
    "    # 6. Pad Sequences to have the same length\n",
    "    max_seq_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "    \n",
    "    # Stratified sampling to maintain label distribution in train/test split\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(stratified_split.split(padded_sequences, labels))\n",
    "    \n",
    "    # Create train/test sequences and labels based on stratified indices\n",
    "    X_train = padded_sequences[train_idx]\n",
    "    X_test = padded_sequences[test_idx]\n",
    "    y_train = labels[train_idx]\n",
    "    y_test = labels[test_idx]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Apply preprocessing with stratified sampling to both datasets\n",
    "complete_X_train, complete_X_test, complete_y_train, complete_y_test = preprocess_with_stratification(complete_data)\n",
    "imputed_X_train, imputed_X_test, imputed_y_train, imputed_y_test = preprocess_with_stratification(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1bcd875-51af-4b1e-8d74-f28396ab6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.base import BaseEstimator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, InputLayer, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class KerasModelWrapper(BaseEstimator):\n",
    "    def __init__(self, learning_rate=0.001, gru_units=64, dropout_rate=0.2, epochs=10, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gru_units = gru_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.input_shape = None  # Will be dynamically set during `fit`\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        if self.input_shape is None:\n",
    "            raise ValueError(\"Input shape must be set before creating the model.\")\n",
    "        model = Sequential([\n",
    "            InputLayer(shape=self.input_shape),\n",
    "            Masking(mask_value=0.0),\n",
    "            GRU(self.gru_units, return_sequences=False, dropout=self.dropout_rate),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy', \n",
    "                               tf.keras.metrics.Precision(name='precision'), \n",
    "                               tf.keras.metrics.Recall(name='recall'),\n",
    "                               tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Dynamically set the input shape based on training data\n",
    "        self.input_shape = (X.shape[1], X.shape[2])\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return binary predictions\n",
    "        probas = self.model.predict(X)\n",
    "        return (probas > 0.5).astype(int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Return f1_score as the scoring metric\n",
    "        y_pred = self.predict(X)\n",
    "        return f1_score(y, y_pred)\n",
    "\n",
    "def tune_model(X_train, y_train, X_test, y_test, tune_type):\n",
    "    # Instantiate the model wrapper\n",
    "    model = KerasModelWrapper()\n",
    "\n",
    "    # Define the parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gru_units': [32, 64, 128],\n",
    "        'dropout_rate': [0.2, 0.3, 0.5],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'epochs': [10]\n",
    "    }\n",
    "\n",
    "    # Define custom F1 scorer\n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "    # RandomizedSearchCV to tune hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, \n",
    "                                       n_iter=10, cv=3, verbose=3, n_jobs=4, \n",
    "                                       random_state=42, scoring=f1_scorer)\n",
    "    # Perform the search\n",
    "    random_search_result = random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(\"Best Hyperparameters:\", random_search_result.best_params_)\n",
    "    print(\"Best F1 Score:\", random_search_result.best_score_)\n",
    "    \n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = random_search_result.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    print(f'Test F1 Score: {test_f1:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    model_save_path = f'{tune_type}_gru_best_model.keras'\n",
    "    best_model.model.save(model_save_path)\n",
    "    print(f\"Best model saved at: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de96ba-1293-4053-b0dc-7dbc72104ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "tune_model(X_train = complete_X_train, y_train = complete_y_train, X_test = complete_X_test, y_test = complete_y_test, tune_type = 'complete_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b5e40a-95db-4dd9-9d8e-5fa98f08a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_model(imputed_X_train, imputed_y_train, imputed_X_test, imputed_y_test, 'imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46ef715-db72-4665-a2da-5961faeb8e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan, 0.9865319728851318, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model and re-evaluate on the test set\n",
    "loaded_model = load_model('imputed_gru_best_model.keras')\n",
    "model_metrics = loaded_model.evaluate(imputed_X_test, imputed_y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badec2bc-911b-4dde-afbe-a82325887835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Change to complete data in model and then tune complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97b79d-b075-4df2-807c-1cabe8738ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90f505-f42a-498c-a623-3ad74ce73c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
