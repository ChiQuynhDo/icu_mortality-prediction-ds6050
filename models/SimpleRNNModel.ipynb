{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a7ea36-6b28-49cc-8d96-f4cb4f3079be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 13:25:11.683228: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-08 13:25:11.697708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 13:25:11.713251: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 13:25:11.718191: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 13:25:11.731185: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30f3c0d-bcf5-49bd-be94-ddffff3e48b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /sfs/gpfs/tardis/home/rmd9ev/Documents/MSDS/DS6050/Project\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "os.chdir(current_path)\n",
    "print(f\"Working directory set to: {current_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7501b12-d42b-45e0-8333-acc984eac6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get all CSV file paths in the specified directory\n",
    "csv_files = glob.glob(\"/sfs/gpfs/tardis/home/rmd9ev/Documents/MSDS/DS6050/Project/final_data/*.csv\")\n",
    "\n",
    "csv_files_sorted = sorted(csv_files)\n",
    "\n",
    "# Read and concatenate all CSV files into a single DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files_sorted]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Inspect the combined DataFrame\n",
    "#combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e321fa-924d-489b-a0ff-69e14181cde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data = combined_df[['insurance','race','marital_status','gender','anchor_age','value','amount','amountuom','label','sequence_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f95254b-8401-4f46-b8d3-26983755e549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(X_data, columns=['insurance','race','marital_status','gender','amountuom','label','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2624db3-0caf-41a3-8bd6-824be76b5f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anchor_age                      0\n",
       "amount                      15856\n",
       "sequence_num                    0\n",
       "insurance_Medicaid              0\n",
       "insurance_Medicare              0\n",
       "                            ...  \n",
       "value_SIMV/PSV/AutoFlow         0\n",
       "value_SPONT                     0\n",
       "value_Sensor Medic (HFO)        0\n",
       "value_Standby                   0\n",
       "value_VOL/AC                    0\n",
       "Length: 104, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NaN values in amount\n",
    "data_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5618542e-d73e-4e3d-94e9-b0fdbe92bf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " data_encoded['amount'] = data_encoded['amount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8bc4071-f66b-40b4-9bde-f68c7dfcb2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88f780ef-01a5-4674-9786-afd654536cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 850479\n",
      "Validation set size: 182245\n",
      "Testing set size: 182246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into features (X) and target labels (y)\n",
    "X = data_encoded.values  # Replace 'input_column' with your actual column name\n",
    "X = X.astype(np.float32)\n",
    "y = combined_df['Died'].values  # Replace 'target_column' with your actual target column\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "# Split data into training, validation, and testing sets (e.g., 70% train, 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65248392-a7b5-43af-8de8-ec74f734fe24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a function to convert arrays to TensorFlow Datasets\n",
    "def to_tf_dataset(X, y, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "#def to_tf_dataset(X, y, batch_size=32):\n",
    "#    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    " #   dataset = dataset.shuffle(buffer_size=len(X)).padded_batch(\n",
    "#        batch_size,\n",
    "#        padded_shapes=([104,], []),\n",
    "#        padding_values=(0.0, 0)\n",
    "#        ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#    return dataset\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = to_tf_dataset(X_train, y_train)\n",
    "val_dataset = to_tf_dataset(X_val, y_val)\n",
    "test_dataset = to_tf_dataset(X_test, y_test)\n",
    "\n",
    "# Inspect the dataset\n",
    "#for x, y in train_dataset.take(1):\n",
    "#    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1950ffbe-bddf-4c61-bf03-e7aca9f931ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  78.    80.  1286.  ...    0.     0.     0. ]\n",
      " [  74.   102.   612.  ...    0.     0.     0. ]\n",
      " [  66.    80.   392.  ...    0.     0.     0. ]\n",
      " ...\n",
      " [  65.    80.    38.  ...    0.     0.     0. ]\n",
      " [  80.     1.5  952.  ...    0.     0.     0. ]\n",
      " [  77.   103.    37.  ...    0.     0.     0. ]], shape=(32, 104), dtype=float32) tf.Tensor([0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0], shape=(32,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 13:29:50.703805: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8dd1b88-60b1-472e-a59e-f8e8ee11335b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 104), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "element_spec = train_dataset.element_spec\n",
    "print(element_spec)\n",
    "#train_dataset.tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b46028b-89d7-43d8-971e-cbd0c169485f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 4ms/step - binary_accuracy: 0.7499 - loss: 2.4055 - val_binary_accuracy: 0.7520 - val_loss: 0.5599\n",
      "Epoch 2/2\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 4ms/step - binary_accuracy: 0.7512 - loss: 0.5611 - val_binary_accuracy: 0.7525 - val_loss: 0.5596\n"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "batch_size = 32\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1,input_shape = (None,1))\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss= keras.losses.BinaryCrossentropy(), \n",
    "              optimizer=optimizer,\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)]\n",
    "             )\n",
    "history = model.fit(train_dataset, epochs=2,\n",
    "                    validation_data=(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "955a9880-ce24-4d1a-8319-a7b7624da7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5696/5696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - binary_accuracy: 0.7509 - loss: 0.5614\n",
      "Loss:  0.5596147179603577\n",
      "Accuracy:  0.7525377869606018\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "281de0e2-28fa-48ad-88f4-25f136207659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 11ms/step - accuracy: 0.7692 - loss: 0.4882 - val_accuracy: 0.7606 - val_loss: 0.4860\n",
      "Epoch 2/2\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 11ms/step - accuracy: 0.7718 - loss: 0.4842 - val_accuracy: 0.7861 - val_loss: 0.4799\n"
     ]
    }
   ],
   "source": [
    "# model 2\n",
    "# Build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(mask_value=0.0, input_shape=(None, 1)),  # Masking layer to handle variable-length sequences\n",
    "    keras.layers.SimpleRNN(32),  # SimpleRNN with 32 units\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Example output layer for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_dataset, epochs=2,\n",
    "                    validation_data=(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc9ef3e3-8eff-4f3c-874c-449879ba9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5696/5696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.7874 - loss: 0.4788\n",
      "Loss:  0.4783785045146942\n",
      "Accuracy:  0.7868814468383789\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ed7e8-923f-4081-b11b-e93919aedc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imbalanced data?\n",
    "#data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acd483-031e-4350-94a3-df35fd9e3ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
