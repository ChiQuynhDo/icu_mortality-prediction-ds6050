{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a7ea36-6b28-49cc-8d96-f4cb4f3079be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:47:03.327883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-08 15:47:03.784416: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-08 15:47:03.953503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-08 15:47:05.079026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30f3c0d-bcf5-49bd-be94-ddffff3e48b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /sfs/gpfs/tardis/home/rmd9ev/Documents/MSDS/DS6050/Project\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "os.chdir(current_path)\n",
    "print(f\"Working directory set to: {current_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7501b12-d42b-45e0-8333-acc984eac6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Get all CSV file paths\n",
    "csv_files = glob.glob(\"/sfs/gpfs/tardis/home/rmd9ev/Documents/MSDS/DS6050/Project/final_data/*.csv\")\n",
    "\n",
    "csv_files_sorted = sorted(csv_files)\n",
    "\n",
    "# Read and concatenate all CSV files into a single DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files_sorted]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Inspect the combined DataFrame\n",
    "#combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8e321fa-924d-489b-a0ff-69e14181cde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data = combined_df[['insurance','race','marital_status','gender','anchor_age','value','amount','amountuom','label','sequence_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f95254b-8401-4f46-b8d3-26983755e549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(X_data, columns=['insurance','race','marital_status','gender','amountuom','label','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2624db3-0caf-41a3-8bd6-824be76b5f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anchor_age                      0\n",
       "amount                      15856\n",
       "sequence_num                    0\n",
       "insurance_Medicaid              0\n",
       "insurance_Medicare              0\n",
       "                            ...  \n",
       "value_SIMV/PSV/AutoFlow         0\n",
       "value_SPONT                     0\n",
       "value_Sensor Medic (HFO)        0\n",
       "value_Standby                   0\n",
       "value_VOL/AC                    0\n",
       "Length: 104, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NaN values in amount\n",
    "data_encoded.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5618542e-d73e-4e3d-94e9-b0fdbe92bf12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " data_encoded['amount'] = data_encoded['amount'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bc4071-f66b-40b4-9bde-f68c7dfcb2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f780ef-01a5-4674-9786-afd654536cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 850479\n",
      "Validation set size: 182245\n",
      "Testing set size: 182246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into features (X) and target labels (y)\n",
    "X = data_encoded.values  \n",
    "X = X.astype(np.float32)\n",
    "y = combined_df['Died'].values  \n",
    "y = y.astype(np.int32)\n",
    "\n",
    "# Split data into training, validation, and testing sets (e.g., 70% train, 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Testing set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65248392-a7b5-43af-8de8-ec74f734fe24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:47:35.432815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46853 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define a function to convert arrays to TensorFlow Datasets\n",
    "def to_tf_dataset(X, y, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "#def to_tf_dataset(X, y, batch_size=32):\n",
    "#    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    " #   dataset = dataset.shuffle(buffer_size=len(X)).padded_batch(\n",
    "#        batch_size,\n",
    "#        padded_shapes=([104,], []),\n",
    "#        padding_values=(0.0, 0)\n",
    "#        ).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#    return dataset\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = to_tf_dataset(X_train, y_train)\n",
    "val_dataset = to_tf_dataset(X_val, y_val)\n",
    "test_dataset = to_tf_dataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1950ffbe-bddf-4c61-bf03-e7aca9f931ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 70.  74.  99. ...   0.   0.   0.]\n",
      " [ 67. 124. 223. ...   0.   0.   0.]\n",
      " [ 79.  80.  78. ...   0.   0.   0.]\n",
      " ...\n",
      " [ 77.  88. 451. ...   0.   0.   0.]\n",
      " [ 87.  11. 351. ...   0.   0.   0.]\n",
      " [ 67.  80.  55. ...   0.   0.   0.]], shape=(32, 104), dtype=float32) tf.Tensor([0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0], shape=(32,), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 15:47:41.791040: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8dd1b88-60b1-472e-a59e-f8e8ee11335b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 104), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "element_spec = train_dataset.element_spec\n",
    "print(element_spec)\n",
    "#train_dataset.tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b46028b-89d7-43d8-971e-cbd0c169485f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmd9ev/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731098886.191180  875957 service.cc:146] XLA service 0x56304da2a560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731098886.191233  875957 service.cc:154]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-11-08 15:48:06.220152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-08 15:48:06.341418: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   43/26578\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 4ms/step - binary_accuracy: 0.7550 - loss: 2.1356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731098886.965833  875957 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 4ms/step - binary_accuracy: 0.7512 - loss: 1.8763 - val_binary_accuracy: 0.7510 - val_loss: 0.5616\n",
      "Epoch 2/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4ms/step - binary_accuracy: 0.7507 - loss: 0.5617 - val_binary_accuracy: 0.7510 - val_loss: 0.5613\n",
      "Epoch 3/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4ms/step - binary_accuracy: 0.7506 - loss: 0.5618 - val_binary_accuracy: 0.7510 - val_loss: 0.5615\n",
      "Epoch 4/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 4ms/step - binary_accuracy: 0.7506 - loss: 0.5617 - val_binary_accuracy: 0.7510 - val_loss: 0.5612\n",
      "Epoch 5/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 4ms/step - binary_accuracy: 0.7509 - loss: 0.5614 - val_binary_accuracy: 0.7510 - val_loss: 0.5615\n",
      "Epoch 6/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 4ms/step - binary_accuracy: 0.7510 - loss: 0.5613 - val_binary_accuracy: 0.7510 - val_loss: 0.5612\n",
      "Epoch 7/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4ms/step - binary_accuracy: 0.7512 - loss: 0.5611 - val_binary_accuracy: 0.7510 - val_loss: 0.5613\n",
      "Epoch 8/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 4ms/step - binary_accuracy: 0.7496 - loss: 0.5629 - val_binary_accuracy: 0.7510 - val_loss: 0.5612\n",
      "Epoch 9/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 4ms/step - binary_accuracy: 0.7502 - loss: 0.5622 - val_binary_accuracy: 0.7510 - val_loss: 0.5612\n",
      "Epoch 10/10\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 4ms/step - binary_accuracy: 0.7505 - loss: 0.5619 - val_binary_accuracy: 0.7510 - val_loss: 0.5613\n"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "batch_size = 32\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1,input_shape = (None,1))\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss= keras.losses.BinaryCrossentropy(), \n",
    "              optimizer=optimizer,\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)]\n",
    "             )\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "955a9880-ce24-4d1a-8319-a7b7624da7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5696/5696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - binary_accuracy: 0.7530 - loss: 0.5593\n",
      "Loss:  0.5597178339958191\n",
      "Accuracy:  0.7525377869606018\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "281de0e2-28fa-48ad-88f4-25f136207659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 10ms/step - accuracy: 0.7691 - loss: 0.4878 - val_accuracy: 0.7860 - val_loss: 0.4799\n",
      "Epoch 2/5\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 10ms/step - accuracy: 0.7742 - loss: 0.4825 - val_accuracy: 0.7846 - val_loss: 0.4761\n",
      "Epoch 3/5\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 10ms/step - accuracy: 0.7754 - loss: 0.4819 - val_accuracy: 0.7830 - val_loss: 0.4791\n",
      "Epoch 4/5\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 10ms/step - accuracy: 0.7742 - loss: 0.4816 - val_accuracy: 0.7570 - val_loss: 0.4921\n",
      "Epoch 5/5\n",
      "\u001b[1m26578/26578\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 10ms/step - accuracy: 0.7722 - loss: 0.4840 - val_accuracy: 0.7853 - val_loss: 0.4760\n"
     ]
    }
   ],
   "source": [
    "# model 2\n",
    "# Build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Masking(mask_value=0.0, input_shape=(None, 1)),  # for variable-length sequences\n",
    "    keras.layers.SimpleRNN(32),  # SimpleRNN with 32 units\n",
    "    keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_dataset, epochs=5,\n",
    "                    validation_data=(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9ef3e3-8eff-4f3c-874c-449879ba9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5696/5696\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - accuracy: 0.7869 - loss: 0.4739\n",
      "Loss:  0.4750058948993683\n",
      "Accuracy:  0.7862943410873413\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ed7e8-923f-4081-b11b-e93919aedc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imbalanced data?\n",
    "#data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acd483-031e-4350-94a3-df35fd9e3ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
