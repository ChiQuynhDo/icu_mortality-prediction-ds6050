{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c3965-c412-45b8-9a02-6111c313bffb",
   "metadata": {},
   "source": [
    "# Predicting Mortality Risk of ICU Patients Post-CABG\n",
    "\n",
    "<span style=\"color:red;\"><b>\n",
    "MODEL BUILDING NOTEBOOK\n",
    "</span></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725f767-3ea1-46ee-985a-353c4ccc2ec4",
   "metadata": {},
   "source": [
    "## Setting up environment and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d2175-f3da-443d-803b-eef7f8f17a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:22:20.655336: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-08 18:22:22.420316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-08 18:22:22.792653: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-08 18:22:22.938141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 18:22:23.924033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, average_precision_score, f1_score\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters, RandomSearch, Objective\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3823e511-6a0b-4968-802c-7f08622975df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility by setting random seeds\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Setting eagerly to true // https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ca9449-9b2a-4121-a56a-6b0f2fb4fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "os.chdir('/sfs/gpfs/tardis/home/krb3ym/Documents/MSDS/DS6050/final_project/Predicting-Mortality-Risk-of-ICU-Patients-Post-Coronary-Artery-Bypass-Graft-Surgery/')\n",
    "\n",
    "# Verifying GPU availability\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Verifying tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be0cdb2-f310-4f3e-a269-434c7d55be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "directory1 = './data/final_data/chunked_complete_data/'\n",
    "directory2 = './data/final_data/chunked_imputed_data/'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "complete_csv_files = [os.path.join(directory1, file) for file in os.listdir(directory1) if file.endswith('.csv')]\n",
    "imputed_csv_files = [os.path.join(directory2, file) for file in os.listdir(directory2) if file.endswith('.csv')]\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "complete_dfs = [pd.read_csv(csv_file) for csv_file in complete_csv_files]\n",
    "imputed_dfs = [pd.read_csv(csv_file) for csv_file in imputed_csv_files]\n",
    "\n",
    "complete_final_df = pd.concat(complete_dfs, ignore_index=True)\n",
    "imputed_final_df = pd.concat(imputed_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8663755-370a-42e3-8c14-4db34dcee683",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_final_df.stay_id = complete_final_df.stay_id.astype(int).astype(str)\n",
    "imputed_final_df.stay_id = imputed_final_df.stay_id.astype(int).astype(str)\n",
    "\n",
    "complete_final_df.time_bucket = pd.to_datetime(complete_final_df.time_bucket)\n",
    "imputed_final_df.time_bucket = pd.to_datetime(imputed_final_df.time_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5517acaa-c645-40f1-958d-08eacc6fb0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>time_bucket</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>anchor_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>insurance</th>\n",
       "      <th>vent</th>\n",
       "      <th>charlson</th>\n",
       "      <th>map</th>\n",
       "      <th>hr</th>\n",
       "      <th>pao2</th>\n",
       "      <th>fio2</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>lactate</th>\n",
       "      <th>platelets</th>\n",
       "      <th>gcs</th>\n",
       "      <th>epinephrine</th>\n",
       "      <th>norepinephrine</th>\n",
       "      <th>phenylephrine</th>\n",
       "      <th>dobutamine</th>\n",
       "      <th>milrinone</th>\n",
       "      <th>dopamine</th>\n",
       "      <th>mortality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30004530</td>\n",
       "      <td>2165-07-31 12:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>DIVORCED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>71.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30004530</td>\n",
       "      <td>2165-07-31 13:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>DIVORCED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>71.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30004530</td>\n",
       "      <td>2165-07-31 14:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>DIVORCED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>70.555556</td>\n",
       "      <td>84.0</td>\n",
       "      <td>197.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30004530</td>\n",
       "      <td>2165-07-31 15:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>DIVORCED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>67.777778</td>\n",
       "      <td>83.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30004530</td>\n",
       "      <td>2165-07-31 16:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>White</td>\n",
       "      <td>DIVORCED</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>71.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>141.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stay_id         time_bucket  seq_num  anchor_age gender   race  \\\n",
       "0  30004530 2165-07-31 12:00:00        1          63      M  White   \n",
       "1  30004530 2165-07-31 13:00:00        2          63      M  White   \n",
       "2  30004530 2165-07-31 14:00:00        3          63      M  White   \n",
       "3  30004530 2165-07-31 15:00:00        4          63      M  White   \n",
       "4  30004530 2165-07-31 16:00:00        5          63      M  White   \n",
       "\n",
       "  marital_status insurance  vent  charlson        map    hr   pao2   fio2  \\\n",
       "0       DIVORCED  Medicare     0         5  73.333333  71.0  305.0   21.0   \n",
       "1       DIVORCED  Medicare     1         5  73.333333  71.0  300.0  100.0   \n",
       "2       DIVORCED  Medicare     1         5  70.555556  84.0  197.5   40.0   \n",
       "3       DIVORCED  Medicare     1         5  67.777778  83.0   95.0   40.0   \n",
       "4       DIVORCED  Medicare     1         5  65.000000  71.0   96.0   40.0   \n",
       "\n",
       "   creatinine  lactate  platelets   gcs  epinephrine  norepinephrine  \\\n",
       "0         1.0      1.3      141.0  15.0          0.0             0.0   \n",
       "1         1.0      1.3      141.0  15.0          0.0             0.0   \n",
       "2         1.0      1.3      141.0  15.0          0.0             0.0   \n",
       "3         1.0      1.3      141.0  15.0          0.0             0.0   \n",
       "4         1.0      1.3      141.0  15.0          0.0             0.0   \n",
       "\n",
       "   phenylephrine  dobutamine  milrinone  dopamine  mortality  \n",
       "0            0.0         0.0        0.0       0.0      False  \n",
       "1            1.0         0.0        0.0       0.0      False  \n",
       "2            0.0         0.0        0.0       0.0      False  \n",
       "3            0.0         0.0        0.0       0.0      False  \n",
       "4            0.0         0.0        0.0       0.0      False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "complete_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ce7fd-0226-4246-ab9a-f08ad1323096",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9972f5-d3db-49d9-aaba-688d6df29b3e",
   "metadata": {},
   "source": [
    "**Creating helper functions and classes for data preprocessing and model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccf77d4-0639-44e4-ad24-a5b23833190e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating F1 score class\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        precision = self.precision.result()\n",
    "        recall = self.recall.result()\n",
    "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9b6069b-c1c0-4cad-a218-9060ba78e0f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create class to preprocess data\n",
    "class Preprocessor:\n",
    "    def __init__(self, pad_length=156, seed = 42):\n",
    "        # Set global seeds\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        keras.utils.set_random_seed(seed)\n",
    "        \n",
    "        # Enable deterministic operations\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "        self.pad_length = pad_length\n",
    "        self.label_encoders = {}\n",
    "        self.numerical_scaler = StandardScaler()\n",
    "        self.numerical_columns = [\"anchor_age\", \"map\", \"hr\", \"pao2\", \"fio2\", \"gcs\", \"charlson\",\n",
    "                                \"creatinine\", \"lactate\", \"platelets\", \"epinephrine\", \n",
    "                                \"norepinephrine\", \"phenylephrine\", \"dobutamine\", \n",
    "                                \"milrinone\", \"dopamine\"]\n",
    "        self.categorical_columns = ['insurance', 'gender', 'race', 'marital_status']\n",
    "        self.binary_columns = ['vent']\n",
    "        self.outcome = ['mortality']\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit preprocessor on training data\"\"\"\n",
    "        # Initialize and fit label encoders for categorical columns\n",
    "        for col in self.categorical_columns:\n",
    "            self.label_encoders[col] = LabelEncoder()\n",
    "            self.label_encoders[col].fit(df[col].astype(str))\n",
    "        \n",
    "        # Fit scaler on numerical columns (reshaping for time series)\n",
    "        numerical_data = df[self.numerical_columns].values\n",
    "        self.numerical_scaler.fit(numerical_data)\n",
    "                \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform the data using fitted preprocessors\"\"\"\n",
    "        # Transform categorical columns\n",
    "        transformed_categorical = df[self.categorical_columns].copy()\n",
    "        for col in self.categorical_columns:\n",
    "            transformed_categorical[col] = self.label_encoders[col].transform(df[col].astype(str))\n",
    "\n",
    "        # Stay ID\n",
    "        stay_id = df['stay_id'].copy()\n",
    "        \n",
    "        # Scale numerical columns\n",
    "        transformed_numerical = pd.DataFrame(\n",
    "            self.numerical_scaler.transform(df[self.numerical_columns]),\n",
    "            columns=self.numerical_columns\n",
    "        )\n",
    "\n",
    "        # Binary columns are used as-is (no transformation)\n",
    "        transformed_binary = df[self.binary_columns].astype(int)\n",
    "        transformed_outcome = df[self.outcome].astype(int)\n",
    "        \n",
    "        # Concatenate data\n",
    "        transformed_data = pd.concat([stay_id, transformed_categorical, transformed_numerical, transformed_binary, transformed_outcome], axis=1)\n",
    "        self.transformed_data = transformed_data\n",
    "        \n",
    "        # Combine transformed categorical, numerical, and binary columns\n",
    "        return transformed_data\n",
    "\n",
    "    def vectorization(self):\n",
    "        \"\"\"Convert transformed data to sequence vectors\"\"\"\n",
    "        # Create sequences\n",
    "        self.sequence_data = []\n",
    "        self.sequence_labels = []\n",
    "        \n",
    "        for stay in self.transformed_data['stay_id'].unique():\n",
    "            stay_data = self.transformed_data[self.transformed_data['stay_id'] == stay]\n",
    "            features = stay_data[self.numerical_columns + self.categorical_columns + self.binary_columns].values\n",
    "            label = stay_data[self.outcome].values[-1]\n",
    "            \n",
    "            self.sequence_data.append(features)\n",
    "            self.sequence_labels.append(label)\n",
    "        \n",
    "        # Pad sequences\n",
    "        self.sequence_data = keras.preprocessing.sequence.pad_sequences(\n",
    "            self.sequence_data, \n",
    "            padding='post',\n",
    "            maxlen=self.pad_length,\n",
    "            value=-99\n",
    "        )\n",
    "\n",
    "        self.sequence_labels = np.array(self.sequence_labels)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def train_test_split(self, size = 0.8):\n",
    "        \"\"\"Split data into training testing\"\"\"\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.sequence_data, self.sequence_labels, test_size=0.2, random_state=42)\n",
    "        return self\n",
    "\n",
    "    def Dataset(self, batch_size = 64):\n",
    "        \"\"\"Create dataset object\"\"\"\n",
    "        if not hasattr(self, 'X_train'):\n",
    "            raise ValueError(\"Must call train_test_split first\")\n",
    "\n",
    "        train_df = tf.data.Dataset.from_tensor_slices((self.X_train, self.y_train))\n",
    "        train_df = (\n",
    "            train_df\n",
    "            .shuffle(buffer_size=len(self.X_train))        # Shuffle the entire dataset in memory\n",
    "            .batch(batch_size)                        # Batch the data\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)   # Prefetch batches for performance\n",
    "        )\n",
    "        \n",
    "        val_df = tf.data.Dataset.from_tensor_slices((self.X_val, self.y_val))\n",
    "        val_df = (\n",
    "            val_df\n",
    "            .batch(batch_size)                        # Batch the data\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE)   # Prefetch batches for performance\n",
    "        )\n",
    "\n",
    "        return train_df, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9513df68-62e9-4849-a3ec-670b4c406a22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define custom wrapper for the Keras model to use with keras tuner\n",
    "class KerasModelWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, class_weight=None, seed = 42, max_epochs=20, max_trials=25):\n",
    "        # Set global seeds\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        self.max_epochs = max_epochs\n",
    "        self.max_trials = max_trials\n",
    "        keras.utils.set_random_seed(seed)\n",
    "        # Enable deterministic operations\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "        self.seed = seed\n",
    "        self.model = None\n",
    "        self.best_hp = None\n",
    "        self.classes_ = np.array([0, 1])  # Binary classification\n",
    "        self.tuner = None\n",
    "        self.class_weight = class_weight  # Store class weights\n",
    "        self.callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True)\n",
    "        ]\n",
    "\n",
    "    def create_model(self, hp, df):\n",
    "        \"\"\"Create the model with optional hyperparameters.\"\"\"\n",
    "        # Define hyperparameters\n",
    "        hp_units = hp.Int('units', min_value=16, max_value=256, step = 16)\n",
    "        hp_dropout = hp.Choice('dropout_rate', [0.2, 0.3, 0.4, 0.5])\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "        # Add seed to layer initialization\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=self.seed)\n",
    "\n",
    "        # Extract the shape of the input data from the Dataset\n",
    "        for X_batch, _ in df.take(1):\n",
    "            input_shape = X_batch.shape[1:]  # Exclude the batch dimension\n",
    "            break\n",
    "        \n",
    "        # Defining model architecture\n",
    "        model = keras.Sequential([\n",
    "                    keras.layers.InputLayer(shape=((input_shape[0],input_shape[1]))),\n",
    "                    keras.layers.Masking(mask_value=-99),\n",
    "                    layers.LSTM(hp_units, return_sequences=False, dropout=hp_dropout, kernel_initializer=initializer, recurrent_initializer=initializer),\n",
    "                    keras.layers.Dense(1, activation=None, kernel_initializer=initializer)\n",
    "                ])\n",
    "\n",
    "        # Defining metrics\n",
    "        metrics = [\n",
    "            'accuracy',\n",
    "            Precision(name='precision'),\n",
    "            Recall(name='recall'),\n",
    "            AUC(name='auc'),\n",
    "            AUC(name='auprc', curve='PR'),\n",
    "            F1Score(name='f1_score')\n",
    "        ]\n",
    "\n",
    "        # Use binary cross-entropy with logits\n",
    "        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        # Compiling model\n",
    "        model.compile(optimizer=Adam(learning_rate=hp_learning_rate), \n",
    "                     loss= loss_fn, \n",
    "                     metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    def compute_class_weight(self, df):\n",
    "        \"\"\"\n",
    "        Compute class weights automatically if not provided\n",
    "        \n",
    "        Args:\n",
    "            train: TensorFlow dataset\n",
    "        \n",
    "        Returns:\n",
    "            dict: Class weights\n",
    "        \"\"\"\n",
    "        # Extract labels from the dataset\n",
    "        #y_true = np.array([label.numpy() for _, label in df])\n",
    "        y_true = np.concatenate([label.numpy().flatten() for _, label in df], axis=0)\n",
    "\n",
    "        # Compute class distribution\n",
    "        unique, counts = np.unique(y_true, return_counts=True)\n",
    "        total_samples = len(y_true)\n",
    "        \n",
    "        # Compute balanced class weights\n",
    "        class_weights = {\n",
    "            0: total_samples / (2 * counts[0]),\n",
    "            1: total_samples / (2 * counts[1])\n",
    "        }\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def tune(self, train, validation, project_name = 'LSTM_tuning'):\n",
    "        \"\"\"Hyperparameter fine tuning using random grid search\"\"\"\n",
    "        # Automatically compute class weights if not provided\n",
    "        if self.class_weight is None:\n",
    "            self.class_weight = self.compute_class_weight(train)\n",
    "\n",
    "        def build_model(hp, df = train):\n",
    "            return self.create_model(hp, df = train)\n",
    "        \n",
    "        tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective=Objective(\"val_auprc\", direction=\"max\"),\n",
    "            max_trials=self.max_trials,\n",
    "            executions_per_trial=1,\n",
    "            directory='models',\n",
    "            project_name=project_name,\n",
    "            seed = self.seed\n",
    "        )\n",
    "    \n",
    "        tuner.search(\n",
    "            train,\n",
    "            validation_data=validation,\n",
    "            epochs=self.max_epochs,\n",
    "            callbacks=self.callbacks,\n",
    "            class_weight=self.class_weight\n",
    "        )\n",
    "    \n",
    "        # Save the best hyperparameters and model\n",
    "        self.best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        self.model = tuner.get_best_models(num_models=1)[0]\n",
    "        self.tuner = tuner\n",
    "    \n",
    "        return self.best_hp, self.model\n",
    "\n",
    "    def retrain_best_model(self, train, validation):\n",
    "        \"\"\"Retrain the best model to obtain its training history.\"\"\"\n",
    "        if self.best_hp is None:\n",
    "            raise ValueError(\"No hyperparameters tuned yet. Please run tune().\")\n",
    "\n",
    "        # Automatically compute class weights if not provided\n",
    "        if self.class_weight is None:\n",
    "            self.class_weight = self.compute_class_weight(train)\n",
    "        \n",
    "        # Rebuild the best model using the best hyperparameters\n",
    "        best_model = self.create_model(self.best_hp, df = train)\n",
    "        if best_model is None:\n",
    "            raise ValueError(\"Failed to create model\")\n",
    "        \n",
    "        # Retrain the model and capture the history\n",
    "        history = best_model.fit(\n",
    "            train,\n",
    "            epochs=self.max_epochs,\n",
    "            validation_data=validation,\n",
    "            callbacks=self.callbacks,\n",
    "            verbose=1,\n",
    "            class_weight=self.class_weight\n",
    "        )\n",
    "        if history is None or history.history.get('loss') is None:\n",
    "            raise RuntimeError(\"Training failed\")\n",
    "        \n",
    "        return best_model, history\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Return predicted class labels.\n",
    "        \n",
    "        Args:\n",
    "            df: TensorFlow dataset\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Predicted class labels\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model has been trained or tuned yet.\")\n",
    "\n",
    "        logits = self.model.predict(df)  # Predict probabilities\n",
    "        probabilities = tf.nn.sigmoid(logits).numpy()\n",
    "        return (probabilities > 0.5).astype(int).flatten()  # Convert to binary class labels (0 or 1)\n",
    "    \n",
    "    def predict_proba(self, df):\n",
    "        \"\"\"Return probability estimates for both classes (negative and positive)\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model has been trained or tuned yet.\")\n",
    "\n",
    "        logits = self.model.predict(df)  # Predict probabilities\n",
    "        probabilities = tf.nn.sigmoid(logits).numpy()\n",
    "        return np.hstack([1 - probabilities, probabilities])  # Class 0 and Class 1 probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8f8b6-6efd-4782-8c32-b5eca38f5dfd",
   "metadata": {},
   "source": [
    "#### Building model on complete data\n",
    "\n",
    "Instantiate the model wrapper, define the parameters for grid tuning, and conduct RandomizedSearchCV to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc92e7f-4a03-47c1-81ea-720d9042e39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:402: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "2024-12-08 18:24:16.196775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31027 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:af:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocessor()\n",
    "\n",
    "# Encoding data\n",
    "preprocess.fit(complete_final_df)\n",
    "data = preprocess.transform(complete_final_df)\n",
    "\n",
    "# Vectorization\n",
    "preprocess.vectorization()\n",
    "\n",
    "# Splitting train/test\n",
    "preprocess.train_test_split()\n",
    "\n",
    "# Creating datasets\n",
    "train, val = preprocess.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b70c6b7-da87-403e-8466-c29c62e0a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 01m 44s]\n",
      "val_auprc: 0.2839135527610779\n",
      "\n",
      "Best val_auprc So Far: 0.6756983399391174\n",
      "Total elapsed time: 00h 24m 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krb3ym/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Initiate Wrapper\n",
    "wrapper = KerasModelWrapper()\n",
    "# Fine tune the model with training and validation set\n",
    "best_hp, best_model = wrapper.tune(train, val, project_name= 'lstm_tuned_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993e1e66-3473-4cf1-9445-cd0b2f814d26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'units': 224, 'dropout_rate': 0.2, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">220,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)            │       \u001b[38;5;34m220,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m225\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">220,641</span> (861.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m220,641\u001b[0m (861.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">220,641</span> (861.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m220,641\u001b[0m (861.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print best parameters\n",
    "print(f\"Best Hyperparameters: {best_hp.values}\")\n",
    "wrapper.model.summary()  # Show the best model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a09a124-0874-4425-b284-37d040e4c827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9948979616165161, 'auc': 0.8873117566108704, 'auprc': 0.7859706878662109, 'f1_score': 0.7777776718139648, 'loss': 0.25695544481277466, 'precision': 0.7777777910232544, 'recall': 0.7777777910232544}\n"
     ]
    }
   ],
   "source": [
    "# Get model metrics\n",
    "best_model.evaluate(val, verbose = 0)\n",
    "print(best_model.get_metrics_result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ebc047-7f19-4d5c-91b7-532096c54d04",
   "metadata": {},
   "source": [
    "**Building model on imputed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb78a899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:402: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/krb3ym/.local/lib/python3.10/site-packages/keras/src/utils/sequence_utils.py:125: RuntimeWarning: invalid value encountered in cast\n",
      "  trunc = np.asarray(trunc, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "# Fit the preprocessor on the new dataset\n",
    "preprocess.fit(imputed_final_df)\n",
    "\n",
    "# Transform the new dataset\n",
    "imputed_df = preprocess.transform(imputed_final_df)\n",
    "\n",
    "# Vectorize the transformed data\n",
    "preprocess.vectorization()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "preprocess.train_test_split()\n",
    "\n",
    "# Create the datasets for training and validation\n",
    "train_imp, val_imp = preprocess.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "472a4146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 19:13:40.671013: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from models/lstm_tuned_imputed/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krb3ym/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Initiate Wrapper\n",
    "imputed_wrapper = KerasModelWrapper()\n",
    "# Fine tune the model with training and validation set\n",
    "best_imp_hp, best_imp_model = imputed_wrapper.tune(train_imp, val_imp, project_name= 'lstm_tuned_imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da4450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'units': 16, 'dropout_rate': 0.4, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ masking (\u001b[38;5;33mMasking\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m156\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,449</span> (9.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,449\u001b[0m (9.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,449</span> (9.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,449\u001b[0m (9.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print best parameters\n",
    "print(f\"Best Hyperparameters: {best_imp_hp.values}\")\n",
    "imputed_wrapper.model.summary()  # Show the best model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "623e7705-5422-4ba9-8996-0d9323c9da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9910714030265808, 'auc': 0.8822222352027893, 'auprc': 0.44508033990859985, 'f1_score': 0.5882352590560913, 'loss': 0.4046884775161743, 'precision': 0.625, 'recall': 0.5555555820465088}\n"
     ]
    }
   ],
   "source": [
    "# Get model metrics\n",
    "best_imp_model.evaluate(val, verbose = 0)\n",
    "print(best_imp_model.get_metrics_result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ffd8519-9d59-4cd5-8b73-8b9460183d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalPreprocessor(Preprocessor):\n",
    "    def __init__(self, window_size=12, prediction_horizon=24, pad_length=156, stride = 6, seed=42):\n",
    "        # Validate inputs first\n",
    "        if not isinstance(window_size, int) or window_size <= 0:\n",
    "            raise ValueError(\"window_size must be positive integer\")\n",
    "        if not isinstance(prediction_horizon, int) or prediction_horizon <= 0:\n",
    "            raise ValueError(\"prediction_horizon must be positive integer\")\n",
    "            \n",
    "        # Initialize parent\n",
    "        super().__init__(pad_length=pad_length, seed=seed)\n",
    "        \n",
    "        # Add temporal attributes\n",
    "        self.time_columns = ['deathtime']\n",
    "        self.window_size = int(window_size)\n",
    "        self.prediction_horizon = int(prediction_horizon)\n",
    "        self._reset_state()\n",
    "        self.stride = int(stride)\n",
    "\n",
    "    def _reset_state(self):\n",
    "        \"\"\"Reset internal state\"\"\"\n",
    "        self.sequence_data = None\n",
    "        self.sequence_labels = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        \"\"\"Validate columns and fit preprocessor\"\"\"\n",
    "        # Get available columns\n",
    "        available_cols = df.columns.tolist()\n",
    "        \n",
    "        # Update column lists to only include available columns\n",
    "        self.categorical_columns = [col for col in self.categorical_columns \n",
    "                                  if col in available_cols]\n",
    "        self.numerical_columns = [col for col in self.numerical_columns \n",
    "                                if col in available_cols]\n",
    "        self.binary_columns = [col for col in self.binary_columns \n",
    "                             if col in available_cols]\n",
    "        \n",
    "        # Validate essential columns\n",
    "        essential_cols = ['stay_id', 'deathtime'] + self.outcome\n",
    "        missing = [col for col in essential_cols if col not in available_cols]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing essential columns: {missing}\")\n",
    "            \n",
    "        return super().fit(df)\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform with available columns only\"\"\"\n",
    "        if not hasattr(self, 'label_encoders'):\n",
    "            raise ValueError(\"Must call fit() before transform()\")\n",
    "            \n",
    "        # Convert deathtime\n",
    "        df['deathtime'] = pd.to_datetime(df['deathtime'], errors='coerce')\n",
    "        df['time_bucket'] = pd.to_datetime(df['time_bucket'])\n",
    "        \n",
    "        # Transform only available columns\n",
    "        parts = []\n",
    "        \n",
    "        # Stay ID\n",
    "        parts.append(df[['stay_id']].copy())\n",
    "        \n",
    "        # Categorical\n",
    "        if self.categorical_columns:\n",
    "            transformed_cat = df[self.categorical_columns].copy()\n",
    "            for col in self.categorical_columns:\n",
    "                transformed_cat[col] = self.label_encoders[col].transform(\n",
    "                    df[col].astype(str))\n",
    "            parts.append(transformed_cat)\n",
    "            \n",
    "        # Numerical\n",
    "        if self.numerical_columns:\n",
    "            transformed_num = pd.DataFrame(\n",
    "                self.numerical_scaler.transform(df[self.numerical_columns]),\n",
    "                columns=self.numerical_columns,\n",
    "                index=df.index\n",
    "            )\n",
    "            parts.append(transformed_num)\n",
    "            \n",
    "        # Binary\n",
    "        if self.binary_columns:\n",
    "            parts.append(df[self.binary_columns].astype(int))\n",
    "            \n",
    "        # Outcome\n",
    "        parts.append(df[self.outcome].astype(int))\n",
    "        \n",
    "        # Deathtime & time bucket\n",
    "        parts.append(df[['deathtime']])\n",
    "        parts.append(df[['time_bucket']])\n",
    "\n",
    "        # Combine all parts\n",
    "        transformed_data = pd.concat(parts, axis=1)\n",
    "        \n",
    "        return transformed_data   \n",
    "\n",
    "    def create_temporal_windows(self, df):\n",
    "        \"\"\"Create sliding windows with labels for temporal prediction using vectorized operations\"\"\"\n",
    "        # Pre-filter stays with sufficient data\n",
    "        stay_counts = df.groupby('stay_id').size()\n",
    "        valid_stays = stay_counts[stay_counts >= self.window_size].index\n",
    "        df_filtered = df[df['stay_id'].isin(valid_stays)]\n",
    "        \n",
    "        # Create window indices using numpy operations\n",
    "        stays = df_filtered.groupby('stay_id')\n",
    "        \n",
    "        windows_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for stay_id, stay_data in stays:\n",
    "            # Convert to numpy for faster operations\n",
    "            values = stay_data[self.numerical_columns + \n",
    "                             self.categorical_columns + \n",
    "                             self.binary_columns].values\n",
    "            \n",
    "            # Create strided array view for windows\n",
    "            n_windows = (len(stay_data) - self.window_size) // self.stride + 1\n",
    "            \n",
    "            # Create strided view of data\n",
    "            strided_shape = (n_windows, self.window_size, values.shape[1])\n",
    "            s0, s1 = values.strides[:2]\n",
    "            \n",
    "            strided_data = np.lib.stride_tricks.as_strided(\n",
    "                values,\n",
    "                shape=strided_shape,\n",
    "                strides=(self.stride * s0, s0, s1),\n",
    "                writeable=False\n",
    "            )\n",
    "\n",
    "            # Calculate labels vectorized\n",
    "            window_end_indices = np.arange(self.window_size-1, \n",
    "                                         len(stay_data), \n",
    "                                         self.stride)[:n_windows]\n",
    "            \n",
    "            window_end_times = stay_data['time_bucket'].iloc[window_end_indices].values\n",
    "            target_times = window_end_times + pd.Timedelta(hours=self.prediction_horizon)\n",
    "            death_time = stay_data['deathtime'].iloc[0]\n",
    "            \n",
    "            if pd.isna(death_time):\n",
    "                labels = np.zeros(n_windows)\n",
    "            else:\n",
    "                labels = (death_time <= target_times).astype(int)\n",
    "            \n",
    "            windows_list.append(strided_data)\n",
    "            labels_list.append(labels)\n",
    "        \n",
    "        if not windows_list:\n",
    "            raise ValueError(\"No valid windows could be created\")\n",
    "        \n",
    "        return np.vstack(windows_list), np.concatenate(labels_list)\n",
    "        \n",
    "    def transform_temporal(self, df):\n",
    "        \"\"\"Transform data with temporal awareness\"\"\"\n",
    "        self._reset_state()\n",
    "        # Basic transformation\n",
    "        transformed_data = self.transform(df)  # Use parent's transform\n",
    "        \n",
    "        # Create temporal windows\n",
    "        self.sequence_data, self.sequence_labels = self.create_temporal_windows(transformed_data)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def prepare_realtime_window(self, current_data):\n",
    "        \"\"\"Prepare the most recent window for real-time prediction\n",
    "        \n",
    "        Args:\n",
    "            current_data: DataFrame with current patient data\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Formatted window ready for model input\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if len(current_data) < self.window_size:\n",
    "            raise ValueError(f\"Not enough data points. Need {self.window_size}, got {len(current_data)}\")\n",
    "\n",
    "        # Transform current data\n",
    "        transformed = self.transform(current_data)\n",
    "        \n",
    "        # Get last window_size hours\n",
    "        recent_window = transformed.iloc[-self.window_size:]\n",
    "        \n",
    "        # Reshape for model input\n",
    "        window_data = recent_window[self.numerical_columns + \n",
    "                                  self.categorical_columns + \n",
    "                                  self.binary_columns].values\n",
    "        \n",
    "        window_data = np.expand_dims(window_data, axis=0)        \n",
    "        \n",
    "        return window_data\n",
    "\n",
    "    # Override vectorization to prevent conflicts\n",
    "    def vectorization(self):\n",
    "        \"\"\"Not used in temporal processing\"\"\"\n",
    "        raise NotImplementedError(\"Use transform_temporal instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d29b3a19-c4d4-44f0-a611-2e350d411fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLSTMWrapper(KerasModelWrapper, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model_type='standard', **kwargs):\n",
    "        self.model_type = model_type\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def create_model(self, hp, df):\n",
    "        \"\"\"Create model based on type\"\"\"\n",
    "        if self.model_type == 'temporal':\n",
    "            return self._create_temporal_model(hp, df)\n",
    "        return self._create_standard_model(hp, df)\n",
    "    \n",
    "    def _create_temporal_model(self, hp, df):\n",
    "        \"\"\"Temporal LSTM model for sliding windows\"\"\"\n",
    "        hp_units = hp.Int('units', min_value=16, max_value=256, step=16)\n",
    "        hp_dropout = hp.Choice('dropout_rate', [0.2, 0.3, 0.4, 0.5])\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "        # Get temporal input shape\n",
    "        for X_batch, _ in df.take(1):\n",
    "            input_shape = X_batch.shape[1:]\n",
    "            break\n",
    "\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.InputLayer(shape=input_shape),\n",
    "            keras.layers.LSTM(\n",
    "                hp_units,\n",
    "                return_sequences=True,  # Required for stacked LSTM\n",
    "                dropout=hp_dropout,\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l2=1e-6),\n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed),\n",
    "                recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed)\n",
    "            ),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(hp_dropout),\n",
    "            keras.layers.LSTM(\n",
    "                hp_units//2,\n",
    "                dropout=hp_dropout,\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l2=1e-6), \n",
    "                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed),\n",
    "                recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=self.seed)\n",
    "            ),    \n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(1, activation=None)\n",
    "        ])\n",
    "\n",
    "        metrics = [\n",
    "            'accuracy', \n",
    "            Precision(name='precision'),\n",
    "            Recall(name='recall'),\n",
    "            AUC(name='auc'),\n",
    "            AUC(name='auprc', curve='PR'),\n",
    "            F1Score(name='f1_score')\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            metrics=metrics\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "42328156-2677-46af-8c07-daae188201ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:402: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:402: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit dataset\n",
    "temporal_prep = TemporalPreprocessor(window_size=12, prediction_horizon=48, stride= 2)\n",
    "temporal_prep.fit(complete_final_df)\n",
    "transformed_data = temporal_prep.transform(complete_final_df)  # Just transform features\n",
    "transformed = temporal_prep.transform_temporal(complete_final_df)\n",
    "temporal_prep.train_test_split()\n",
    "train_rltime, val_rltime = temporal_prep.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "504be0d6-1d36-4489-9c95-55625c3024bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def inspect_dataset_windows(dataset):\n",
    "    \"\"\"Inspect sliding windows in dataset including positive event counts\"\"\"\n",
    "    print(\"\\nSliding Window Analysis:\")\n",
    "    \n",
    "    try:\n",
    "        total_windows = 0\n",
    "        total_positives = 0\n",
    "        \n",
    "        # Analyze all batches\n",
    "        for batch_x, batch_y in dataset:\n",
    "            total_windows += batch_x.shape[0]\n",
    "            total_positives += tf.reduce_sum(batch_y).numpy()\n",
    "            \n",
    "            # Print first batch details\n",
    "            if total_windows == batch_x.shape[0]:  # First batch\n",
    "                print(f\"\\nBatch size: {batch_x.shape[0]}\")\n",
    "                print(f\"Window size: {batch_x.shape[1]}\")\n",
    "                print(f\"Features per timestep: {batch_x.shape[2]}\")\n",
    "                \n",
    "                # Show example window\n",
    "                example = batch_x[0]\n",
    "                print(\"\\nExample Window:\")\n",
    "                print(f\"First timestep: {example[0]}\")\n",
    "                print(f\"Last timestep: {example[-1]}\")\n",
    "                print(f\"Output: {batch_y[0]}\")\n",
    "        \n",
    "        # Print overall statistics\n",
    "        print(f\"\\nTotal windows: {total_windows}\")\n",
    "        print(f\"Positive events: {total_positives}\")\n",
    "        print(f\"Event rate: {(total_positives/total_windows)*100:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting dataset: {str(e)}\")\n",
    "\n",
    "def analyze_window_distribution(df, window_size, stride=6):\n",
    "    \"\"\"Analyze distribution of windows and events per patient\"\"\"\n",
    "    try:\n",
    "        windows_per_patient = {}\n",
    "        events_per_patient = {}\n",
    "        \n",
    "        for stay_id, stay_data in df.groupby('stay_id'):\n",
    "            n_windows = (len(stay_data) - window_size) // stride + 1\n",
    "            if n_windows > 0:\n",
    "                windows_per_patient[stay_id] = n_windows\n",
    "                events_per_patient[stay_id] = stay_data['mortality'].iloc[-1]\n",
    "        \n",
    "        windows = np.array(list(windows_per_patient.values()))\n",
    "        events = np.array(list(events_per_patient.values()))\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats_summary = {\n",
    "            'min_windows': np.min(windows),\n",
    "            'max_windows': np.max(windows),\n",
    "            'mean_windows': np.mean(windows),\n",
    "            'std_windows': np.std(windows),\n",
    "            'total_events': np.sum(events),\n",
    "            'event_rate': np.mean(events) * 100\n",
    "        }\n",
    "        \n",
    "        print(\"\\nDistribution Summary:\")\n",
    "        for stat, value in stats_summary.items():\n",
    "            print(f\"{stat}: {value:.2f}\")\n",
    "        \n",
    "        print(\"\\nWindow Quantiles:\")\n",
    "        quantiles = np.percentile(windows, [10, 25, 50, 75, 90, 99])\n",
    "        for q, v in zip([10, 25, 50, 75, 90, 99], quantiles):\n",
    "            print(f\"{q}th percentile: {v:.2f}\")\n",
    "            \n",
    "        return windows_per_patient, events_per_patient\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing distribution: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def run_window_analysis(train_ds, val_ds, transformed_data, window_size=12):\n",
    "    \"\"\"Run complete window analysis for both datasets\"\"\"\n",
    "    print(\"Training Dataset:\")\n",
    "    inspect_dataset_windows(train_ds)\n",
    "    windows_train, events_train = analyze_window_distribution(transformed_data, window_size)\n",
    "    \n",
    "    print(\"\\nValidation Dataset:\")\n",
    "    inspect_dataset_windows(val_ds)\n",
    "    windows_val, events_val = analyze_window_distribution(transformed_data, window_size)\n",
    "    \n",
    "    return (windows_train, events_train), (windows_val, events_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a288a59c-ed15-4d7b-9ec5-170d762927c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "\n",
      "Sliding Window Analysis:\n",
      "\n",
      "Batch size: 64\n",
      "Window size: 12\n",
      "Features per timestep: 21\n",
      "\n",
      "Example Window:\n",
      "First timestep: [ 0.48349178 -0.42317584 -0.28042401 -0.73267301 -0.87649828  0.27651707\n",
      "  0.59770333  0.32879551 -0.308316   -0.31986234 -0.17836889 -0.11762086\n",
      " -0.212277   -0.08384237  1.56760226 -0.05252904  1.          1.\n",
      "  5.          1.          0.        ]\n",
      "Last timestep: [ 0.48349178 -0.28110242  0.53957507 -0.61035676 -0.87649828  0.27651707\n",
      "  0.59770333  0.32879551 -0.12934513 -0.24528047 -0.17836889 -0.11762086\n",
      " -0.212277   -0.08384237  5.40794587 -0.05252904  1.          1.\n",
      "  5.          1.          0.        ]\n",
      "Output: 0.0\n",
      "\n",
      "Total windows: 102925\n",
      "Positive events: 969.0\n",
      "Event rate: 0.94%\n",
      "\n",
      "Distribution Summary:\n",
      "min_windows: 1.00\n",
      "max_windows: 333.00\n",
      "mean_windows: 11.29\n",
      "std_windows: 18.71\n",
      "total_events: 57.00\n",
      "event_rate: 1.46\n",
      "\n",
      "Window Quantiles:\n",
      "10th percentile: 3.00\n",
      "25th percentile: 3.00\n",
      "50th percentile: 6.00\n",
      "75th percentile: 11.00\n",
      "90th percentile: 22.50\n",
      "99th percentile: 95.85\n",
      "\n",
      "Validation Dataset:\n",
      "\n",
      "Sliding Window Analysis:\n",
      "\n",
      "Batch size: 64\n",
      "Window size: 12\n",
      "Features per timestep: 21\n",
      "\n",
      "Example Window:\n",
      "First timestep: [-0.60708051  1.06859502  0.06124227 -0.74626371  1.4364203   0.27651707\n",
      " -0.61943506 -0.69238419  0.29708037  0.14893795 -0.17836889 -0.11762086\n",
      "  0.7639929  -0.08384237 -0.20377395 -0.05252904  1.          0.\n",
      "  3.          1.          1.        ]\n",
      "Last timestep: [-0.60708051  0.9265216   0.3345753  -0.92294275  1.30792482  0.27651707\n",
      " -0.61943506 -0.73810866  0.30216773 -0.11742585 -0.17836889 -0.11762086\n",
      "  0.37348494 -0.08384237 -0.20377395 -0.05252904  1.          0.\n",
      "  3.          1.          1.        ]\n",
      "Output: 0.0\n",
      "\n",
      "Total windows: 25732\n",
      "Positive events: 231.0\n",
      "Event rate: 0.90%\n",
      "\n",
      "Distribution Summary:\n",
      "min_windows: 1.00\n",
      "max_windows: 333.00\n",
      "mean_windows: 11.29\n",
      "std_windows: 18.71\n",
      "total_events: 57.00\n",
      "event_rate: 1.46\n",
      "\n",
      "Window Quantiles:\n",
      "10th percentile: 3.00\n",
      "25th percentile: 3.00\n",
      "50th percentile: 6.00\n",
      "75th percentile: 11.00\n",
      "90th percentile: 22.50\n",
      "99th percentile: 95.85\n"
     ]
    }
   ],
   "source": [
    "windows_train, windows_val = run_window_analysis(train_rltime, val_rltime, transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "51c54527-fc21-40e7-a8cb-7d75b182ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 03m 11s]\n",
      "val_auprc: 0.053677454590797424\n",
      "\n",
      "Best val_auprc So Far: 0.13953542709350586\n",
      "Total elapsed time: 00h 45m 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krb3ym/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters at 0x7f74e6bc5f30>,\n",
       " <Sequential name=sequential, built=True>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create temporal model\n",
    "sliding_model = TemporalLSTMWrapper(model_type= 'temporal', max_epochs = 30)\n",
    "sliding_model.tune(train_rltime, val_rltime, project_name= 'sliding_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ae8c7-dd51-44b8-9652-3eae59738496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveTemporalPreprocessor(TemporalPreprocessor):\n",
    "    def create_progressive_windows(self, df, max_hours=100):\n",
    "        \"\"\"Create windows of increasing size from admission\"\"\"\n",
    "        windows = []\n",
    "        labels = []\n",
    "        \n",
    "        for stay_id, stay_data in df.groupby('stay_id'):\n",
    "            # Get admission time\n",
    "            admission_time = stay_data.index[0]\n",
    "            death_time = stay_data['deathtime'].iloc[0]\n",
    "            \n",
    "            # For each hour threshold\n",
    "            for hour in range(1, max_hours + 1):\n",
    "                cutoff_time = admission_time + pd.Timedelta(hours=hour)\n",
    "                window_data = stay_data[stay_data.index <= cutoff_time]\n",
    "                \n",
    "                if len(window_data) > 0:\n",
    "                    # Get features\n",
    "                    features = window_data[self.numerical_columns + \n",
    "                                        self.categorical_columns + \n",
    "                                        self.binary_columns].values\n",
    "                    \n",
    "                    # Determine outcome\n",
    "                    label = 1 if (pd.notna(death_time) and death_time <= cutoff_time) else 0\n",
    "                    \n",
    "                    windows.append(features)\n",
    "                    labels.append(label)\n",
    "        \n",
    "        return np.array(windows), np.array(labels)\n",
    "\n",
    "def evaluate_progressive_prediction(model, preprocessor, test_df, max_hours=100):\n",
    "    \"\"\"Evaluate model performance over increasing time windows\"\"\"\n",
    "    aucs = []\n",
    "    \n",
    "    for hour in range(1, max_hours + 1):\n",
    "        # Get data up to hour\n",
    "        X, y = preprocessor.create_progressive_windows(test_df, max_hours=hour)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        # Calculate AUC\n",
    "        auc = roc_auc_score(y, y_pred)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    return aucs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.13.0",
   "language": "python",
   "name": "tensorflow-2.13.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
