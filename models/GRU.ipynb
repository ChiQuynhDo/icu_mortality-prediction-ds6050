{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84aca87f-fe47-4e42-bf02-baaf0a4833af",
   "metadata": {},
   "source": [
    "# GRU Model\n",
    "## LOAD IN TWO DATA SETS FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4947340d-03cb-4ad8-8c38-0cd14c6500be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 11:01:47.070806: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n",
      "2024-12-10 11:01:47.070902: I tensorflow/core/platform/cpu_feature_guard.cc:192] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Masking, InputLayer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import GRU, Dense, Masking, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import BaseEstimator\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import HyperParameters, RandomSearch, Objective\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from functools import partial\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Local Path\n",
    "data_path = r'../data'\n",
    "\n",
    "# Files to Read\n",
    "complete_data_files = [i for i in listdir(data_path) if ('model' in i) & ('complete' in i)]\n",
    "imputed_data_files = [i for i in listdir(data_path) if ('model' in i) & ('impute' in i)]\n",
    "\n",
    "# Read and concat data\n",
    "complete_data = pd.concat([pd.read_csv(data_path + '/' + file) for file in complete_data_files]).reset_index(drop=True).drop(columns=['time_bucket'])\n",
    "imputed_data = pd.concat([pd.read_csv(data_path + '/' + file) for file in imputed_data_files]).reset_index(drop=True).drop(columns=['time_bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a541ad1-43bb-4c20-b870-bd32be4e3041",
   "metadata": {},
   "source": [
    "### Preprocess with Stratification for Imbalanced Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bb6ccd-d6cb-43aa-bcfc-92761975a9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_with_stratification(data):\n",
    "    # Sort and group data by 'Unique Stay' and 'sequence_num' to ensure time order\n",
    "    df = data.sort_values(by=['stay_id', 'seq_num'])\n",
    "    \n",
    "    # 2. Normalize Numerical Columns\n",
    "    scaler = MinMaxScaler()\n",
    "    numerical_cols = ['map', 'hr', 'pao2', 'fio2', 'creatinine', 'lactate', 'platelets', 'gcs']\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 3. Encode Categorical Variables\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    categorical_cols = ['gender', 'race', 'marital_status', 'insurance']\n",
    "    encoded_cats = encoder.fit_transform(df[categorical_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "    df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "    \n",
    "    # 4. Group by stay_id and Create Sequences\n",
    "    grouped = df.groupby('stay_id')\n",
    "    sequences = [group.drop(columns=['stay_id', 'mortality']).values for _, group in grouped]\n",
    "    labels = [group['mortality'].iloc[0] for _, group in grouped]\n",
    "    \n",
    "    # 5. Convert Labels to NumPy array for Stratified Splitting\n",
    "    labels = np.array(labels).astype('float32')\n",
    "    \n",
    "    # 6. Pad Sequences to have the same length\n",
    "    max_seq_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post')\n",
    "    \n",
    "    # Stratified sampling to maintain label distribution in train/test split\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(stratified_split.split(padded_sequences, labels))\n",
    "    \n",
    "    # Create train/test sequences and labels based on stratified indices\n",
    "    X_train = padded_sequences[train_idx]\n",
    "    X_test = padded_sequences[test_idx]\n",
    "    y_train = labels[train_idx]\n",
    "    y_test = labels[test_idx]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Apply preprocessing with stratified sampling to both datasets\n",
    "complete_X_train, complete_X_test, complete_y_train, complete_y_test = preprocess_with_stratification(complete_data)\n",
    "imputed_X_train, imputed_X_test, imputed_y_train, imputed_y_test = preprocess_with_stratification(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a1117-5c78-4e28-817f-81093fe99816",
   "metadata": {},
   "source": [
    "#### Create Model and Tune Based on F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1bcd875-51af-4b1e-8d74-f28396ab6d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.base import BaseEstimator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, InputLayer, Masking\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "class KerasModelWrapper(BaseEstimator):\n",
    "    def __init__(self, learning_rate=0.001, gru_units=64, dropout_rate=0.2, epochs=10, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gru_units = gru_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.input_shape = None  # Will be dynamically set during `fit`\n",
    "\n",
    "    def create_model(self):\n",
    "\n",
    "        if self.input_shape is None:\n",
    "            raise ValueError(\"Input shape must be set before creating the model.\")\n",
    "        model = Sequential([\n",
    "            InputLayer(input_shape=self.input_shape),\n",
    "            Masking(mask_value=0.0),\n",
    "            GRU(self.gru_units, return_sequences=False, dropout=self.dropout_rate),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=self.learning_rate), \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy', \n",
    "                               tf.keras.metrics.Precision(name='precision'), \n",
    "                               tf.keras.metrics.Recall(name='recall'),\n",
    "                               tf.keras.metrics.AUC(name='auc')])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Dynamically set the input shape based on training data\n",
    "        self.input_shape = (X.shape[1], X.shape[2])\n",
    "        self.model = self.create_model()\n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return binary predictions\n",
    "        probas = self.model.predict(X)\n",
    "        return (probas > 0.12).astype(int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Return f1_score as the scoring metric\n",
    "        y_pred = self.predict(X)\n",
    "        return f1_score(y, y_pred, average='weighted')\n",
    "\n",
    "def tune_model(X_train, y_train, X_test, y_test, tune_type):\n",
    "    # Instantiate the model wrapper\n",
    "    model = KerasModelWrapper()\n",
    "\n",
    "    # Define the parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'gru_units': [32, 64, 128],\n",
    "        'dropout_rate': [0.2, 0.3, 0.5],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'epochs': [10]\n",
    "    }\n",
    "\n",
    "    # Create a partial function to fix the threshold at 0.3\n",
    "    custom_f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "    # RandomizedSearchCV to tune hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, \n",
    "                                       n_iter=10, cv=3, verbose=3, n_jobs=1, \n",
    "                                       random_state=42, scoring=custom_f1_scorer)\n",
    "    # Perform the search\n",
    "    random_search_result = random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and score\n",
    "    print(\"Best Hyperparameters:\", random_search_result.best_params_)\n",
    "    print(\"Best F1 Score:\", random_search_result.best_score_)\n",
    "    \n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = random_search_result.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    print(f'Test F1 Score: {test_f1:.4f}')\n",
    "    \n",
    "    # Save the best model\n",
    "    model_save_path = f'{tune_type}_gru_best_model.keras'\n",
    "    best_model.model.save(model_save_path)\n",
    "    print(f\"Best model saved at: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de96ba-1293-4053-b0dc-7dbc72104ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune on Complete Dataset and Save\n",
    "tune_model(X_train = complete_X_train, y_train = complete_y_train, X_test = complete_X_test, y_test = complete_y_test, tune_type = 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5e40a-95db-4dd9-9d8e-5fa98f08a976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune on Imputed Dataset and Save\n",
    "tune_model(imputed_X_train, imputed_y_train, imputed_X_test, imputed_y_test, 'imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87efc5-8314-48fb-a125-04fed8e3ab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
